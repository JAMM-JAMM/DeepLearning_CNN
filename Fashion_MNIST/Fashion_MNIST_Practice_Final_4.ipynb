{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d8c0a99",
   "metadata": {},
   "source": [
    "### Fashion MNIST\n",
    "- Functional API\n",
    "- Transform the pixel value from 1 ~ 255 to 0 ~ 1 as float32\n",
    "- Apply One Hot Encoding on Label (categorical crossentropy)\n",
    "- Divide Train/Validation/Test\n",
    "- Compile, fit, evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44593a6d",
   "metadata": {},
   "source": [
    "### Building Model using Functional API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "347c171f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init Plugin\n",
      "Init Graph Optimizer\n",
      "Init Kernel\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Layer, Input, Dense, Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "INPUT_SIZE = 28\n",
    "\n",
    "def create_model():\n",
    "    input_tensor = Input(shape=(INPUT_SIZE, INPUT_SIZE))\n",
    "    x = Flatten()(input_tensor)\n",
    "    x = Dense(100, activation='relu')(x)\n",
    "    x = Dense(30, activation='relu')(x)\n",
    "    output = Dense(10, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs=input_tensor, outputs=output)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60550cb7",
   "metadata": {},
   "source": [
    "### Dividing Dataset & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ddd8a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def get_preprocessed_data(images, labels):\n",
    "    \n",
    "    images = np.array(images/255.0, dtype=np.float32)\n",
    "    labels = np.array(labels, dtype=np.float32)\n",
    "    \n",
    "    return images, labels\n",
    "\n",
    "def get_preprocessed_ohe(images, labels):\n",
    "    \n",
    "    images, labels = get_preprocessed_data(images, labels)\n",
    "    \n",
    "    oh_labels = to_categorical(labels)\n",
    "    \n",
    "    return images, oh_labels\n",
    "\n",
    "def get_train_valid_test_set(train_images, train_labels, test_images, test_labels, valid_size=0.15, random_state=2021):\n",
    "    \n",
    "    train_images, train_oh_labels = get_preprocessed_ohe(train_images, train_labels)\n",
    "    test_images, test_oh_labels = get_preprocessed_ohe(test_images, test_labels)\n",
    "    \n",
    "    tr_images, val_images, tr_oh_labels, val_oh_labels = train_test_split(train_images, train_oh_labels, test_size=valid_size, random_state=random_state)\n",
    "    \n",
    "    return (tr_images, tr_oh_labels), (val_images, val_oh_labels), (test_images, test_oh_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c300ff48",
   "metadata": {},
   "source": [
    "### Load Fashion MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e425bf65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train images shape:  (60000, 28, 28)\n",
      "train labels shape:  (60000,)\n",
      "test images shape:  (10000, 28, 28)\n",
      "test labels shape:  (10000,)\n",
      "\n",
      "train images shape:  (51000, 28, 28)\n",
      "train ohe labels shape:  (51000, 10)\n",
      "validatioin images shape:  (9000, 28, 28)\n",
      "validation ohe labels shape:  (9000, 10)\n",
      "test images shape:  (10000, 28, 28)\n",
      "test ohe labels shape:  (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
    "\n",
    "print('train images shape: ', train_images.shape)\n",
    "print('train labels shape: ', train_labels.shape)\n",
    "print('test images shape: ', test_images.shape)\n",
    "print('test labels shape: ', test_labels.shape)\n",
    "\n",
    "print()\n",
    "\n",
    "(tr_images, tr_oh_labels), (val_images, val_oh_labels), (test_images, test_oh_labels) = \\\n",
    "    get_train_valid_test_set(train_images, train_labels, test_images, test_labels, valid_size=0.15, random_state=2021)\n",
    "\n",
    "print('train images shape: ', tr_images.shape)\n",
    "print('train ohe labels shape: ', tr_oh_labels.shape)\n",
    "print('validatioin images shape: ', val_images.shape)\n",
    "print('validation ohe labels shape: ', val_oh_labels.shape)\n",
    "print('test images shape: ', test_images.shape)\n",
    "print('test ohe labels shape: ', test_oh_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ab708c",
   "metadata": {},
   "source": [
    "### Compile, Fit, Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53548450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 100)               78500     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 30)                3030      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                310       \n",
      "=================================================================\n",
      "Total params: 81,840\n",
      "Trainable params: 81,840\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-13 14:34:29.726517: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2021-08-13 14:34:29.726853: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "model = create_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e939fc4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=Adam(0.001), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3bf6e3a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-13 14:34:33.541725: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2021-08-13 14:34:33.544421: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2021-08-13 14:34:33.671864: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "392/399 [============================>.] - ETA: 0s - loss: 0.6084 - accuracy: 0.7953"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-13 14:34:36.283103: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "399/399 [==============================] - 3s 6ms/step - loss: 0.6058 - accuracy: 0.7960 - val_loss: 0.4758 - val_accuracy: 0.8351\n",
      "Epoch 2/20\n",
      "399/399 [==============================] - 2s 6ms/step - loss: 0.4116 - accuracy: 0.8564 - val_loss: 0.4024 - val_accuracy: 0.8558\n",
      "Epoch 3/20\n",
      "399/399 [==============================] - 2s 6ms/step - loss: 0.3743 - accuracy: 0.8675 - val_loss: 0.3953 - val_accuracy: 0.8572\n",
      "Epoch 4/20\n",
      "399/399 [==============================] - 2s 6ms/step - loss: 0.3459 - accuracy: 0.8770 - val_loss: 0.3842 - val_accuracy: 0.8614\n",
      "Epoch 5/20\n",
      "399/399 [==============================] - 2s 6ms/step - loss: 0.3313 - accuracy: 0.8811 - val_loss: 0.3677 - val_accuracy: 0.8656\n",
      "Epoch 6/20\n",
      "399/399 [==============================] - 2s 6ms/step - loss: 0.3119 - accuracy: 0.8875 - val_loss: 0.3471 - val_accuracy: 0.8770\n",
      "Epoch 7/20\n",
      "399/399 [==============================] - 2s 6ms/step - loss: 0.3012 - accuracy: 0.8910 - val_loss: 0.3391 - val_accuracy: 0.8738\n",
      "Epoch 8/20\n",
      "399/399 [==============================] - 2s 6ms/step - loss: 0.2881 - accuracy: 0.8950 - val_loss: 0.3305 - val_accuracy: 0.8792\n",
      "Epoch 9/20\n",
      "399/399 [==============================] - 2s 6ms/step - loss: 0.2781 - accuracy: 0.8986 - val_loss: 0.3417 - val_accuracy: 0.8783\n",
      "Epoch 10/20\n",
      "399/399 [==============================] - 2s 6ms/step - loss: 0.2700 - accuracy: 0.9020 - val_loss: 0.3189 - val_accuracy: 0.8846\n",
      "Epoch 11/20\n",
      "399/399 [==============================] - 2s 6ms/step - loss: 0.2622 - accuracy: 0.9034 - val_loss: 0.3235 - val_accuracy: 0.8837\n",
      "Epoch 12/20\n",
      "399/399 [==============================] - 2s 6ms/step - loss: 0.2535 - accuracy: 0.9061 - val_loss: 0.3274 - val_accuracy: 0.8822\n",
      "Epoch 13/20\n",
      "399/399 [==============================] - 2s 6ms/step - loss: 0.2464 - accuracy: 0.9111 - val_loss: 0.3208 - val_accuracy: 0.8866\n",
      "Epoch 14/20\n",
      "399/399 [==============================] - 2s 6ms/step - loss: 0.2414 - accuracy: 0.9117 - val_loss: 0.3193 - val_accuracy: 0.8850\n",
      "Epoch 15/20\n",
      "399/399 [==============================] - 2s 6ms/step - loss: 0.2366 - accuracy: 0.9125 - val_loss: 0.3337 - val_accuracy: 0.8822\n",
      "Epoch 16/20\n",
      "399/399 [==============================] - 2s 6ms/step - loss: 0.2295 - accuracy: 0.9149 - val_loss: 0.3248 - val_accuracy: 0.8840\n",
      "Epoch 17/20\n",
      "399/399 [==============================] - 2s 6ms/step - loss: 0.2242 - accuracy: 0.9165 - val_loss: 0.3385 - val_accuracy: 0.8814\n",
      "Epoch 18/20\n",
      "399/399 [==============================] - 2s 6ms/step - loss: 0.2171 - accuracy: 0.9201 - val_loss: 0.3219 - val_accuracy: 0.8853\n",
      "Epoch 19/20\n",
      "399/399 [==============================] - 2s 6ms/step - loss: 0.2128 - accuracy: 0.9223 - val_loss: 0.3160 - val_accuracy: 0.8898\n",
      "Epoch 20/20\n",
      "399/399 [==============================] - 2s 6ms/step - loss: 0.2074 - accuracy: 0.9229 - val_loss: 0.3306 - val_accuracy: 0.8886\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x=tr_images, y=tr_oh_labels, batch_size=128, epochs=20, validation_data=(val_images, val_oh_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086b50a4",
   "metadata": {},
   "source": [
    "### Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a8c14e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD4CAYAAADiry33AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8rg+JYAAAACXBIWXMAAAsTAAALEwEAmpwYAAAxGklEQVR4nO3deXxU9b3/8dcnG1khK1sWEgiyqBAxRBQX1Kq4olYtbm2plVq1au/tvXrbX3vb2/bW3tv2Xnu19VqlaosLbhWVqq1XRRRl0YR9CSGQELKTfc98fn+cCYQwCQNZJpn5PB+P88jMnO+Z+c5heM93vud7vkdUFWOMMf4ryNcVMMYYM7gs6I0xxs9Z0BtjjJ+zoDfGGD9nQW+MMX4uxNcV8CQxMVHT09N9XQ1jjBkxNm7cWKmqSZ7WDcugT09PZ8OGDb6uhjHGjBgisq+3ddZ1Y4wxfs6C3hhj/JwFvTHG+Llh2UfvSXt7O8XFxbS0tPi6KoMuPDyclJQUQkNDfV0VY4wfGDFBX1xcTExMDOnp6YiIr6szaFSVqqoqiouLycjI8HV1jDF+YMR03bS0tJCQkODXIQ8gIiQkJATELxdjzNAYMUEP+H3IdwmU92mMGRojpuvGGGNGuk6XUt/STm2z5yVIhLsumDLgr2tB76Wamhqee+457r777hPa7oorruC5554jNjZ2cCpmjPEpVeVgbQt5RTXsrWqktrmdOk9B3tROfWsHfV0CJClmlAW9L9XU1PC73/3umKDv7OwkODi41+1WrVo12FUzxgyhmqY2NhXXkldUQ15xDXnFtVTUtx5eHxYcxOiIUMZEhDAmIpSk6FFkJkUzJiLUWSLDjtzusYSHDk5vuldBLyILgUeAYOBJVX24x/o4YBkwBWgBvqGqW0QkFXgWGA+4gCdU9ZEBrP+Qeeihh9izZw9ZWVmEhoYSHR3NhAkTyM3NZdu2bVx77bUUFRXR0tLC/fffz9KlS4Ej0zk0NDRw+eWXc+655/LJJ5+QnJzM66+/TkREhI/fmTGmNy3tnWwtqSW3qJZNxTXkFdVQWNV0eP2UpCjOy0xkdmoss1NjmTo2msiw4GF3nO24QS8iwcBjwCVAMbBeRFaq6rZuxb4P5KrqdSIy3V3+YqAD+EdV/VxEYoCNIvK3HtuesJ+8sZVtJXX9eYpjzJw4mn+9+tRe1z/88MNs2bKF3NxcPvjgA6688kq2bNlyeAjksmXLiI+Pp7m5mblz5/LlL3+ZhISEo55j9+7dPP/88/zhD3/gpptu4pVXXuG2224b0PdhjDk5nS5ld3k9eUU15BY5LfadZfV0upy+lgljwpmVMoab5qYyOyWW01PGMDp8ZJzr4k2LPgfIV9UCABF5AVgEdA/rmcAvAFR1h4iki8g4VT0IHHQ/Xi8i24HkHtuOSDk5OUeNc//tb3/La6+9BkBRURG7d+8+JugzMjLIysoC4Mwzz6SwsHCoqmuM8UBVyS2q4fXcEt7afPBwF8zo8BBmp8by7elTmJUyhtmpsYwbHe7j2p48b4I+GSjqdr8YOKtHmTzgemCNiOQAk4AUoKyrgIikA2cAn3l6ERFZCiwFSEtL67NCfbW8h0pUVNTh2x988AF///vfWbt2LZGRkSxYsMDjOPhRo0Ydvh0cHExzc/OQ1NUYc7RdZfW8nnuAN/IOsr+6ibDgIC6cnsRlp47njLQ40hMih133S394E/Se3m3P48YPA4+ISC6wGfgCp9vGeQKRaOAV4AFV9djnoqpPAE8AZGdn93Fc2jdiYmKor6/3uK62tpa4uDgiIyPZsWMHn3766RDXzhhzPEXVTazMK+GNvBJ2lNYTJDA/M5HvXJTJZaeNHzHdMCfDm6AvBlK73U8BSroXcIf3EgBxvgb3uhdEJBQn5Jer6qsDUGefSEhIYP78+Zx22mlEREQwbty4w+sWLlzI448/zqxZs5g2bRrz5s3zYU2NMV0q6lt5a1MJK/NK+Hx/DQBz0mL5yTWncsXpE0iKGdX3E/gJ0b4GdQIiEgLswjm4egBYD9yiqlu7lYkFmlS1TUTuBM5T1a+6Q/8ZoFpVH/C2UtnZ2drzwiPbt29nxowZ3j7FiBdo79eYgVLX0s7bW0p5I6+Ej/MrcSlMHx/DNVkTuXrWRFLjI31dxUEhIhtVNdvTuuO26FW1Q0TuBd7BGV65TFW3ishd7vWPAzOAZ0WkE+dA6x3uzecDtwOb3d06AN9XVRtcbozpN1WltK6FgopGCioaWJNfyfs7K2jrcJEWH8ndCzK5Jmsip4yL8XVVfcqrcfTuYF7V47HHu91eC0z1sN0aPPfxG2OM15raOpwwr3QCfY872PdWNtLU1nm4XFLMKG49K41rZk8kKzXWrw6o9oedGWuMGRZcLuVATfPhMHeC3fl7sPbIKDYRSI6NYHJSNHPT45mSFMXkpGgmJ0UxfnS4hbsHFvTGGJ9o63Cx+UAN6wsPsX5vNRv2HaK2uf3w+pjwECYnRXP25AQmdwvz9IQowkN7n3bEHMuC3hgzJOpb2tm47xAbCg+xrrCavKIaWjtcAExOimLhqeOZnRp7uIWeGB1mrfMBYkFvjBkU5XUtrCusdoJ9bzU7SutwKQQHCadNHM1t8yYxNz2e7PQ4EqMDY5ijr1jQD5Lo6GgaGhooKSnhvvvu4+WXXz6mzIIFC/jVr35FdrbHEVHGDHuqSn1rB9UNbVQ1tpFfXs+6vYdYX1jN/mpn8q+I0GDmTIrlOxdNJScjnqzUWKJGWfQMJdvbg2zixIkeQ96Y4cjlUmqa26lubKW60flb1dh2OMir3Ytzu5VDje20dbqOeo74qDDmpsfx1bOdFvvMiaMJDR5RF7PzOxb0XnrwwQeZNGnS4fnof/zjHyMirF69mkOHDtHe3s7PfvYzFi1adNR2hYWFXHXVVWzZsoXm5maWLFnCtm3bmDFjhs11YwZUp0upamyloaWDhtYOGlo6qDt8u52G1g7qWzuob+k4qkx9awcNre3Ut3RQ19yOq5dzKGNGhRAfHUZ8VBjJseGcnjya+KhRJEQ5j8VHh5EWH8nkxCjrWx9mRmbQ//UhKN08sM85/nS4/OFeVy9evJgHHnjgcNCvWLGCt99+m+9+97uMHj2ayspK5s2bxzXXXNPrh/z3v/89kZGRbNq0iU2bNjFnzpyBfQ8mILW0d/LSxmIe/2APB2r6bjyMCgkiJjyE6FEhRLv/psRFEDMqhuhw50IZ8V3B7V4SokYRFxXKqBAb6TJSjcyg94EzzjiD8vJySkpKqKioIC4ujgkTJvDd736X1atXExQUxIEDBygrK2P8+PEen2P16tXcd999AMyaNYtZs2YN5Vswfqa5rZPn1u3nidV7KKtrZU5aLEvPn0xsZKgT5O4wjxkVejjUw0KsCyUQjcyg76PlPZhuuOEGXn75ZUpLS1m8eDHLly+noqKCjRs3EhoaSnp6usfpibuzn7SmvxpaO3h2bSFPfbSXqsY2zp6cwH/dlMXZUxLs82U8GplB7yOLFy/mzjvvpLKykg8//JAVK1YwduxYQkNDef/999m3b1+f259//vksX76cCy+8kC1btrBp06YhqrnxB7VN7Tz9SSHLPt5LbXM7F5ySxHcuyiQ7Pd7XVTPDnAX9CTj11FOpr68nOTmZCRMmcOutt3L11VeTnZ1NVlYW06dP73P7b3/72yxZsoRZs2aRlZVFTk7OENXcjGRVDa08tWYvz67dR0NrB5fMHMd3LspkVkqsr6tmRojjTlPsCzZNceC9X3Os8roWnlhdwPLP9tPS0ckVp0/g3gszmTFhtK+rZoahfk1TbIwZWgdqmvnfD/fwwvoiOl3KoqyJ3L0gk8yx0b6umhmhLOiNGQZUlR2l9TzzSSGvfF4MwA1npnDXBVOYlBB1nK2N6duICnpVDYhRBcOxO80MvIO1zXycX8Wa3RV8vKeKivpWwkKCuCUnjaUXTCE5NsLXVTR+YsQEfXh4OFVVVSQk+PcQMlWlqqqK8PBwX1fFDLC6lnY+K6jm4/xKPtpdwZ6KRgASo8OYn5nI/MxELpw2NmCuY2qGzogJ+pSUFIqLi6moqPB1VQZdeHg4KSkpvq6G6ae2Dhe5RTWsya9kze4K8opr6XQp4aFBnJWRwOK5aZw7NZFp42IICvLfxovxvRET9KGhoWRkZPi6Gsb0SlXZVeZct/Tj/Eo+Laiiqa2TIIFZKbF8+4IpzM9MZM6kWJtOwAypERP0xgwHja0dFB1qoqi6maLqpsO3iw81UVTdRKP7+qUZiVFcPyeZczOTOHtyAmMiQ31ccxPILOiN6aa900VJTbMT5Iea2F/d5A70Zoqrm6hqbDuqfGRYMKlxkaTGR3L2lASmj49hfmYiKXGRPnoHxhzLgt4EvD0VDazadJBVW0rZ6b4KUpeQICE5LoLUuEguPXUcKe5QT4uPJDUugvgou9ydGf68CnoRWQg8AgQDT6rqwz3WxwHLgClAC/ANVd3izbbG+EJXuL+1+SA7SusBOHNSHHcvyCQtIdLdSo9g/OhwQuyiGWaEO27Qi0gw8BhwCVAMrBeRlaq6rVux7wO5qnqdiEx3l7/Yy22NGRIFFQ285SHcf3jVTK44fTwTxti4deOfvGnR5wD5qloAICIvAIuA7mE9E/gFgKruEJF0ERkHTPZiW2MGTUFFA6s2H+TNTRbuJnB5E/TJQFG3+8XAWT3K5AHXA2tEJAeYBKR4uS0AIrIUWAqQlpbmTd2N8agr3N/aXMr2g3WAhbsJbN4EvacjTT3P0X8YeEREcoHNwBdAh5fbOg+qPgE8Ac7slV7Uy5jDiqqbWJlXwpubDlq4G9ODN0FfDKR2u58ClHQvoKp1wBIAcYYg7HUvkcfb1piTVVHfyqrNB3k99wCf768BYE5arIW7MT14E/TrgakikgEcABYDt3QvICKxQJOqtgHfBFarap2IHHdbY05EfUs772wt4/XcA3ycX4lLYfr4GB5cOJ2rZ0+w8evGeHDcoFfVDhG5F3gHZ4jkMlXdKiJ3udc/DswAnhWRTpwDrXf0te3gvBXjr1raO/lgZzmv55bw3o5y2jpcpMZHcPeCTK7Jmsgp42J8XUVjhrURc4UpE1g6Ol2sLahiZW4Jb28ppb61g8ToMK6aNZFrsiZyRmqsnahkTDd2hSkzIqgquUU1vJ7rHFStbGglelQIC08bz6KsiZw9OcFOXjLmJFjQG58rrW3hlc+LeWlDEYVVTYSFBHHRtLEsyprIhdPHEh5qMz0a0x8W9MYnWjs6eW97OSs2FLF6VwUuhbMy4rn7wkwWnjae0eE226MxA8WC3gyp7QfrWLGhiL98cYBDTe1MGBPO3QsyueHMFNIT7dqoxgwGC3oz6Gqb21mZe4AVG4rZfKCW0GDh0pnjuTE7hfOmJhFsV1cyZlBZ0JtB4XIpawuqWLGhiLe3lNLa4WL6+Bj+9eqZLMpKJj4qzNdVNCZgWNCbAVV8qImXNxbz0oZiDtQ0Mzo8hK/MTeWm7FROnTjahkQa4wMW9KbfDjW28fbWUt7IK2FtQRUA52Ym8uDl07l05jgbNWOMj1nQm5NS19LOu1vLeCOvhI/zK+lwKRmJUdx/8VRuODPFpiIwZhixoDdea2zt4O/by3gj7yCrd1XQ1ukiOTaCb543matmTbCuGROYVKHoM/jsf6FiB4xOhjEpEJsKY7qWFIiZAMG+iVwLetOnlvZO3t9RzpubDvLejjJa2l2MGz2K2+ZN4urZE8iyqQhMoOpohS2vwmePw8FcCB8DaWdD/UEo+Ryaqo4uL8EweqIT+l3h3/PLYFT0oFTVgt4co7Wjk492VfLmphL+tq2MxrZOEqPDuPHMVK6aNYG56fEE2ZBIE6jqy2DDMmdpLIfEaXDlb2D2Ygjrdi5IWyPUHoDa/VBb7Cw1Rc7fok9hawm4Oo5+7tg0eGDzgFfZgt4ctqGwmhfXF/H21lLqWzoYExHK1bMnctWsicybHG/zzJjAdmCj0z2z5VVwtcPUy2DeXTD5QvD0qzYsCpJOcRZPXJ1QX+r+Eihyls72Qam6Bb2hqqGVn7+1nVe/OED0qBAuPXUcV8+ayPzMRMJCLNxNAOtsh22vOwFfvA7CYmDuHZCzFBKm9O+5g4JhTLKzeL7C6oCxoA9gqspLG4r5979up7G1g3svzOSeCzOJCLPhkCOKywUF7ztdCfUHISETEqZCYqZzO34KhNkoqBPSWAkbn4b1T0F9CcRPhoW/hKxbIHy0r2t3wizoA1R+eT3ff20L6/ZWMzc9jp9fd7pdwGOkaamF3Odh/R+gKh+ikmDsDCj8GDa9eHTZManuL4BMSJx65O/oFAiyX22HlW52Dq5uegk6W51umav/GzIvGdH7yYI+wLS0d/K79/P5/Yd7iAgN5uHrT+em7FQ7uDqSlG1zwj3vRWhvhJS5cP0fYOYiCBnllGlrhKo9ULXb+Vu527md9wK01R95rpBwp8Wf6P4VMP50yDgfIuN98958oaUOtr4GucudYZKhkXDGrZDzLRg73de1GxAW9AHkk/xKfvCXLeytbOTarIn84MqZJMWM8nW1hofmQ7D6V1BdAFGJTuvY0xIZ7/StDrXODtj5Fqz7AxR+BMGj4PQbYO43IXnOseXDomDCLGfpThUayp3Qr9zt/BKoyofSLbD9TdBOQGDiGTDlQqdFm5pz5AvEX7hcsG8NfLEctq+E9iZIPAUu+SnMuR0i4nxdwwFllxIMAFUNrfx81XZe/fwAkxIi+dm1p3He1CRfV2t4UIW85+HdH0JzNSRNd8Y/N1a6Q68ngcgEd/C7vxCixx65PTrZ6c+NTYPgAZhTv6EcNj4DG/8IdQdgTJpzMPCM2yEqof/P311HG5R84fT37/k/KN7g7IPQSJg0/0jwj53heZTJiVCFxgoo3wbl24/8VYVJ50D6eZA2b+D7ww/tg9znIO85qNkPo0bDaV+GM26D5DP7/758qK9LCVrQ+zFV5aWNxfz7Kudg67fOn8K9F2Xa3DNdSrfAqu/B/rWQkgNX/vpIC9jlgpYaJ4y6loaKo+83Vh653Vp39HNLsHMyTPzko5e4DIhLh9Dw3uul6oTsuiecLgVXuxOwOUvhlMuG7hdFSy0UroE97zvhX5XvPB49HiYvgCkXOX9jxh3/ecp39Aj1bUefUBSZAGNnOuPKizc471mCYMJs50umK/gjYk/8fbQ1Oa32L/7s/BpCYPIFkHUbzLgKQiNO/DmHIQv6AJRf3sD3X9tsB1s9aamDD37hDJmLiIUv/QSybu3fwbb2Fifw6w443T/dl6oCaK3tVljcLf+MHl8EGXBwkxPwB3OdoXxZtzjdM72NxR5KNUXu1v77UPCB8wsIYOypR1r70UlHt9DLtzvjw7uERTu/CMbOcIJ97Axn++huvzDbmqB4Pez72PmiKV4PnW1O8I8/3Qn9SfNh0tm9d7F0TUvwxZ9h61+c4xJxGc6/8+zFzpewn7GgDyAt7Z387oM9/P6DfCJCg/n+FTPsYGsXVdjyCrzzA2gogzO/Dhf/aPAPPKo6xwB6fgFUF0D1XmiqPLp80nTIuRNmfQVGDdMvZ5cLSjcd6ebZ/6kTxl2CQiFpWrdAd4f6mNQT/0Jtb3Za+YVrnPAvWueMiEFg/GlO8Kef60w/0NHidMXlPuf8AgmNglOvc74wJ50zortmjqffQS8iC4FHgGDgSVV9uMf6McCfgTScA7y/UtU/utd9F/gmoMBmYImqtvT1ehb0J87lUj7cXcG/vbHNDrZ6Ur7D6aYp/AgmZMFVv3H6ZIeDllon8KsLnP7+SfNHXiC1NcH+T5xfS2NnOicTDcQxCk/aW5yzVAvXOAdUi9Y5AY84+01dzj7MutUZiTRI88cMN/0KehEJBnYBlwDFwHrgZlXd1q3M94ExqvqgiCQBO4HxQBKwBpipqs0isgJYpapP9/WaFvTeK69r4aWNxazYUMS+qiY72NpTawOs/g9Y+5jTbXDxj5yWvC9GzpjB0dHqDv6PnZCfdaPTFRZg+gp6b4ZX5gD5qlrgfrIXgEXAtm5lFIgRZxrDaKAa6JqtJwSIEJF2IBIoOal3YQ7rdCmrd1Xw/Lr9vLejnE6XMm9yPP9wySlcdup4O9gKTnfJ9pXw9r84/eZZt8ElP3FGxxj/EjLK6ZaZdI6vazJseRP0yUC3oykUc+zEDI8CK3FCPAb4iqq6gAMi8itgP9AMvKuq73p6ERFZCiwFSEtLO5H3EDAO1DSzYn0RL20ooqS2hcToML55XgaL56aRkRh1/CcYzlSdvt66km7j1t1DFk/09P2qPbDqn2DPezDuNLhhmTNiw5gA5U3Qe+os7NnfcxmQC1wETAH+JiIf4fTpLwIygBrgJRG5TVX/fMwTqj4BPAFO142X9fd77Z0u3ttezgvr9/PhrgoAzpuaxA+vmsnFM8b5x6RjhWvg7z9xJo3yJDTKCf3osUd/AfT8QoiIc+Z7+fgR54SihQ/D3Dt9drEHY4YLb/4HFAPdxyKlcGz3yxLgYXU6/PNFZC8wHZgE7FXVCgAReRU4B+fArelDYWUjL24o4qUNxVQ2tDJ+dDjfuTCTG7NTSY33kwmqDubBe/8G+X93rr5z1X87w/Qaq3ofr16z3+mP7fWEJuD0G+HSn0HM+CF9O8YMV94E/XpgqohkAAeAxcAtPcrsBy4GPhKRccA0oADn18A8EYnE6bq5GLCjrL1o7ejk7S2lvLCuiLUFVQQHCRdOG8vNOalccEqS/8wHX7UH/u9nsPVVCI91TjvPufPIiStx6cd/Dk8nNDVWOuOsrZvGmKMcN+hVtUNE7gXewemKWaaqW0XkLvf6x4GfAk+LyGaccH9QVSuBShF5Gfgc5+DsF7i7Z8zR9lY28tVln1FU3UxKXATfu/QUbsxOZdzoPs6gHEgttbD8RmeY3PQrnWX86QM7zK+uBD78JXz+J+cA2nnfg3O+c3JnOwYFOePfI+Od8drGmF7ZCVPDwNaSWr62bB0uhV/fOJsLTkka2hOcOlrhz192pgKYOMc5ExF15muZfpUT+qnzTr6vu6ka1vyXc8anqxOyvwHnf8/pczfGDIj+Dq80g2h9YTXf+ON6YsJDePaOs8gcO8Qnd7hc8Nq3nBOJrvtf5/TwhgrY9VfY8ZZz4YVPfwcR8TDtcif0p1zk3fwgrQ3w6e/hk99Ca73z3Ase8q5rxhgzYCzofej9HeV8e/lGJsZG8Kc7ziI5dognV1KFd/7FmTjrkn9zghiceUfmfNVZWhucYYrb33SW3OXObIZTLnJa+6dcduwUAh1tztV5Vv+H03c+7Uq46P/BuJlD+/6MMYAFvc+8nnuAf1yRx/QJMTyzJIeEaB9MVfDxI87VdObdA+fc57nMqGjnNPKZi5zrZxaucVr6O96CHW86szROOgdmXA2nLHS6f97/uTM6ZtK5sPg5Zz5zY4zPWB+9D/xpbSE/WrmVnPR4nvxaNjHhgzQnSF9yn4e/3OXMxX39kyc+0ZSqM3f5jjed0K/YcWTdhNnOVANTLh55c7YYM0JZH/0woao8+n/5/Ppvu/jSjLE8essc30xXsPvvsPJeyLgArv39yU3PK+Jc2Sh5jhPqlfmw+13nivbTrx7R19c0xt9Y0A8Rl0v5+artPLVmL9efkcwvb5hFqC/GxRdvhBW3O1PGfuXPA3eJuMRMZzHGDDsW9EOgo9PFQ69u5uWNxXz9nHR+dNVM38wPX7UHnrvRmS7g1lcG/jJtxphhyYJ+kLW0d3Lf81/w7rYyvvulU7jv4kzEF/3W9WXwp+uc27e/dvzLvxlj/IYF/SBqaO3gzmc2sLagih9fPZOvz8/wTUVa6mD5Dc5Qx6+/6VwUwhgTMCzoB0l1YxtL/riOLSV1/NdXZnPdGSm+qUhHG7x4m3MNz5tfHD5XVTLGDBkL+kFwsLaZ259aR1F1E/9725l8aaaPuklcLvjLt2Hvh3Dt4zD1S76phzHGpyzoB1hBRQO3P7WOuuZ2nv1GDmdNTvBdZf72Q9jyMnzpx5B1s+/qYYzxKQv6AdQ1OZkqPL90Hqclj/FdZT75H1j7KJx1F8x/wHf1MMb4nAX9ANlRWsfNT3xK9KgQ/vTNs5iS5MMrz+e9CO/+Pzj1OrjsF3Z2qjEBzoJ+AOyvauL2p9YRGRbCi98627dXgMp/D16/G9LPc2ajtDNUjQl4lgL9VF7fwm1PfUZ7p4s/3ZHj25A/8Dm8eDskzYDFywfurFdjzIhmLfp+qG1u56tPraOyoZXl3zyLqeNihr4SLpczl/zGPzrTCMdMgNtehnAfHh8wxgwrFvQnqbmtkzueXs+eigaWfX0uZ6TFDW0FGiqcueE/fwaqC5xrr+bcCWffYxfFNsYcxYL+JLR3urh7+UY27j/EY7fM4bypSUPzwi4XFK52Luqx/U1wtUPaObDgX2DGNRA6RNeXNcaMKBb0J8jlUr73Uh7v76zg3687nStOnzD4L9pb6/3Mr9uFsY0xx2VBfwJUlZ+8sZXXc0v4p8umcctZaYP3YtZ6N8YMEAv6E/DIe7t5Zu0+7jwvg7sXDNLEYD1b7xFxkLMUzvyatd6NMSfFq6AXkYXAI0Aw8KSqPtxj/Rjgz0Ca+zl/pap/dK+LBZ4ETgMU+Iaqrh2oNzBUnvmkkP/++25uODOF718xY2CnGlZ1Rs5sWHak9T5pvrXejTED4rhBLyLBwGPAJUAxsF5EVqrqtm7F7gG2qerVIpIE7BSR5arahvMF8baq3iAiYYAPB5qfnNdzD/CvK7dyycxxPHz96QMX8s01kPe8E/CVu6zv3RgzKLxp0ecA+apaACAiLwCLgO5Br0CMOAkYDVQDHSIyGjgf+DqAO/jbBqz2Q+D9HeX844o8zsqI539uPoOQgbj8X8kXsP4p2PwydDRDcrZz7dZTr4PQiP4/vzHGdONN0CcDRd3uFwNn9SjzKLASKAFigK+oqktEJgMVwB9FZDawEbhfVRt7voiILAWWAqSlDeJBzhOwvrCaby/fyPQJMTz5tez+Xci7rQm2vuoEfMnnEBoJs26CuXfAhNkDV2ljjOnBm6D31E+hPe5fBuQCFwFTgL+JyEfu558DfEdVPxORR4CHgB8e84SqTwBPAGRnZ/d8/iG3/WAd33h6PRPHRPD0khxiwkNP7okqdztdM7nLoaUWkqbD5f8Js79iZ68aY4aEN0FfDKR2u5+C03LvbgnwsKoqkC8ie4HpwH6gWFU/c5d7GSfoh7V9VY3c/tQ6okeF8OwdOSRGn+CcMZ3tsOMt2PAU7F0NQaEw8xrIvgMmnWOzSRpjhpQ3Qb8emCoiGcABYDFwS48y+4GLgY9EZBwwDShQ1UoRKRKRaaq6011mG8NYeZ0zSVmny8ULS88mJc7LY8edHXBoL2xaAZ8/Cw2lMCYNLv4RnHE7RI8d3IobY0wvjhv0qtohIvcC7+AMr1ymqltF5C73+seBnwJPi8hmnK6eB1W10v0U3wGWu0fcFOC0/oel2qZ2bn9qHVUNbTx35zwyx/aYpKyjDWr2O+Pbey41+8DVAQhMvQTm/hYyvwRB/ejXN8aYASBOb8vwkp2drRs2bBjy173tyc/I21vKM9cmMSfm0LFhXlsM6jqyQVgMxGdA/GT3kgEZF0DcpCGvuzEmsInIRlXN9rTOzox1a2hu4Vv7/oHzQrfAW91WhI+B+CmQkgOzFncL9ckQlWj97caYYc+C3q1y7XLOC97C/sxbSZt14ZEWemS8r6tmjDH9YkEP0NlB/Mbfst2VRuTCX0KiDy4gYowxg8QuJQiw5RVGNxbyO72B1HgfXtTbGGMGgbXoXZ2w+j/ZH5rBvvgLCQqyPndjjH+xFv2WV6FqN4+5vszU8XamqjHG/wR20Ls64cNf0pk4gxWNWUwbb902xhj/E9hBv/U1qNpNwan3oAQxdZwdhDXG+J/ADXp3a56kGXwafi4A0yzojTF+KHCDfutrzsU+LvhndpU1EjMqhAlj7EpOxhj/E5hB7x5pQ9J0mHktu8rqmTouemAvD2iMMcNEYAb9tr9AxQ644J9REXaV1TNtvHXbGGP8U+AFvcsFH/7H4dZ8ZUMbh5raOcX6540xfirwgr6rNX/+P0FQMLvK6gEs6I0xfiuwgr6rNZ84zbkQN7Cz1ILeGOPfAivot6+Eiu1wwT8fviDI7vJ64iJDSYwO83HljDFmcARO0Ltczrj5xFMOt+bBadGfMi7GRtwYY/xW4AT9jjegfBucf6Q1r6rsLmuwETfGGL8WGEHvcsEHv4SEqXDa9YcfPljbQn1rh019YIzxa4ER9DvehPKtR/XNA+x0j7ixqQ+MMf7M/4O+a6RNQiac9uWjVu06POLGZq00xvgv/w/6nW9B2eaj+ua77CprYGzMKGIjbcSNMcZ/eRX0IrJQRHaKSL6IPORh/RgReUNE8kRkq4gs6bE+WES+EJE3B6riXlF1RtrETzmmNQ/Y1AfGmIBw3KAXkWDgMeByYCZws4jM7FHsHmCbqs4GFgC/FpHuzeT7ge0DUuMTseMtKN3s9M0HH33VRJdL2V1ebydKGWP8njct+hwgX1ULVLUNeAFY1KOMAjHiDEaPBqqBDgARSQGuBJ4csFp743BrfjKcdsMxq4sONdHS7rL+eWOM3/Mm6JOBom73i92PdfcoMAMoATYD96uqy73uv4F/Blz0QUSWisgGEdlQUVHhRbWOY+dfoXSTM6dN8LHXQLepD4wxgcKboPd0yqj2uH8ZkAtMBLKAR0VktIhcBZSr6sbjvYiqPqGq2aqanZSU5EW1+nwy+OAXEJcBp9/kscju8gYAG0NvjPF73gR9MZDa7X4KTsu9uyXAq+rIB/YC04H5wDUiUojT5XORiPy537U+nl1v99maB6dFnxwbQfQoz+uNMcZfeBP064GpIpLhPsC6GFjZo8x+4GIAERkHTAMKVPVfVDVFVdPd2/2fqt42YLX3pHtrftZXei1mI26MMYHiuEGvqh3AvcA7OCNnVqjqVhG5S0Tuchf7KXCOiGwG3gMeVNXKwap0n3a9Awfz4Pzv9dqab+90UVDRyFQ7EGuMCQBe9Vuo6ipgVY/HHu92uwS49DjP8QHwwQnX8ESowocPQ1x6n635fVWNtHW6bOoDY0xA8K8zY3e/CyVfwHnfg+DQXovtKnMOxNqIG2NMIPCfoFeFDx6G2Ekwe3GfRXeW1hMkkDnWum6MMf7Pf4actNZBVCJkL+mzNQ/OgdhJCVGEhwb3Wc4YY/yB/wR9+Bi49SWnZX8cu8rq7YxYY0zA8J+umy7HuSRgS3snhVVN1j9vjAkY/hf0x1FQ0UinSy3ojTEBI+CCfne5+6pSdrKUMSZABFzQ7yytJyRISE+I8nVVjDFmSARc0O8qq2dyUhRhIQH31o0xASrg0m5XWYPNWGmMCSgBFfRNbR3sr26yqQ+MMQEloII+v9ymPjDGBJ6ACvojV5Wyk6WMMYEjoIJ+V1k9YSFBTLIRN8aYABJgQd/A1LHRBAf1ffasMcb4kwAL+nrrnzfGBJyACfra5nYO1rZY0BtjAk7ABH3+4akP7ECsMSawBEzQ7yx1hlZOHWstemNMYAmYoN9VVk9UWDDJsRG+rooxxgypgAr6qeNiCLIRN8aYABNQQW8nShljApFXQS8iC0Vkp4jki8hDHtaPEZE3RCRPRLaKyBL346ki8r6IbHc/fv9AvwFvVDW0UtnQZiNujDEB6bhBLyLBwGPA5cBM4GYRmdmj2D3ANlWdDSwAfi0iYUAH8I+qOgOYB9zjYdtBt6vM5rgxxgQub1r0OUC+qhaoahvwArCoRxkFYkREgGigGuhQ1YOq+jmAqtYD24HkAau9l3aV2VWljDGBy5ugTwaKut0v5tiwfhSYAZQAm4H7VdXVvYCIpANnAJ+dbGVP1q6yesZEhDI2ZtRQv7QxxvicN0HvaZiK9rh/GZALTASygEdFZPThJxCJBl4BHlDVOo8vIrJURDaIyIaKigovquW9rgOxzg8OY4wJLN4EfTGQ2u1+Ck7LvbslwKvqyAf2AtMBRCQUJ+SXq+qrvb2Iqj6hqtmqmp2UlHQi76FPqsrOUpvjxhgTuLwJ+vXAVBHJcB9gXQys7FFmP3AxgIiMA6YBBe4++6eA7ar6m4GrtvfK61upa+mw/nljTMA6btCragdwL/AOzsHUFaq6VUTuEpG73MV+CpwjIpuB94AHVbUSmA/cDlwkIrnu5YpBeSe96LrYiE19YIwJVCHeFFLVVcCqHo893u12CXCph+3W4LmPf8h0jbixk6WMMYHK78+M3VVWT2L0KBKibcSNMSYw+X3Q7yxrsNa8MSag+XXQu1xKvl1VyhgT4Pw66A/UNNPY1mlBb4wJaH4d9EemPrCuG2NM4PLzoHdfVcpa9MaYAObnQV/PhDHhjA4P9XVVjDHGZ/w66G3qA2OM8eOg73Qp+RUNNvWBMSbg+W3Q76tqpK3DxdSxdiDWGBPY/Dbouw7EWoveGBPo/Djo6xGBTGvRG2MCnN8G/c6yelLjIokM82reNmOM8Vt+G/S7beoDY4wB/DTo2zpcFFQ02hmxxhiDnwb93spGOlxqLXpjjMFPg/7IxUYs6I0xxm+DPjhImJwU5euqGGOMz/lt0KcnRDIqJNjXVTHGGJ/z06C3qQ+MMaaL3wV9S3snhVWNTB1rQW+MMeCHQZ9f3oCqTX1gjDFd/C7obcSNMcYczaugF5GFIrJTRPJF5CEP68eIyBsikiciW0VkibfbDrSdZfWEBQeRnhA52C9ljDEjwnGDXkSCgceAy4GZwM0iMrNHsXuAbao6G1gA/FpEwrzcdkDtLmtgclIUIcF+92PFGGNOijdpmAPkq2qBqrYBLwCLepRRIEZEBIgGqoEOL7cdUDtL661/3hhjuvEm6JOBom73i92PdfcoMAMoATYD96uqy8ttARCRpSKyQUQ2VFRUeFn9ozW0dnCgptn6540xphtvgl48PKY97l8G5AITgSzgUREZ7eW2zoOqT6hqtqpmJyUleVGtY+22A7HGGHMMb4K+GEjtdj8Fp+Xe3RLgVXXkA3uB6V5uO2COjLixWSuNMaaLN0G/HpgqIhkiEgYsBlb2KLMfuBhARMYB04ACL7cdMLvKGggPDSI1zkbcGGNMl+NefklVO0TkXuAdIBhYpqpbReQu9/rHgZ8CT4vIZpzumgdVtRLA07aD81acFv0p42IICvLUY2SMMYHJq+vsqeoqYFWPxx7vdrsEuNTbbQfLztJ6zpt6cv37xhjjr/xmsHlHp4tzpyZy3tREX1fFGGOGFb+5cnZIcBC/uSnL19Uwxphhx29a9MYYYzyzoDfGGD9nQW+MMX7Ogt4YY/ycBb0xxvg5C3pjjPFzFvTGGOPnLOiNMcbPiarHWYN9SkQqgH0nuXkiUDmA1RloVr/+sfr1j9Wvf4Zz/Sapqsc5YIZl0PeHiGxQ1Wxf16M3Vr/+sfr1j9Wvf4Z7/XpjXTfGGOPnLOiNMcbP+WPQP+HrChyH1a9/rH79Y/Xrn+FeP4/8ro/eGGPM0fyxRW+MMaYbC3pjjPFzIzLoRWShiOwUkXwRecjDehGR37rXbxKROUNcv1QReV9EtovIVhG530OZBSJSKyK57uVHQ1zHQhHZ7H7tDR7W+2wfisi0bvslV0TqROSBHmWGdP+JyDIRKReRLd0eixeRv4nIbvffuF627fPzOoj1+08R2eH+93tNRGJ72bbPz8Ig1u/HInKg27/hFb1s66v992K3uhWKSG4v2w76/us3VR1RC85FxvcAk4EwIA+Y2aPMFcBfcS5UPg/4bIjrOAGY474dA+zyUMcFwJs+3I+FQGIf6326D3v8e5finAzis/0HnA/MAbZ0e+w/gIfctx8CftlL/fv8vA5i/S4FQty3f+mpft58Fgaxfj8GvufFv79P9l+P9b8GfuSr/dffZSS26HOAfFUtUNU24AVgUY8yi4Bn1fEpECsiE4aqgqp6UFU/d9+uB7YDyUP1+gPEp/uwm4uBPap6smdKDwhVXQ1U93h4EfCM+/YzwLUeNvXm8zoo9VPVd1W1w333UyBloF/XW73sP2/4bP91EREBbgKeH+jXHSojMeiTgaJu94s5NkS9KTMkRCQdOAP4zMPqs0UkT0T+KiKnDm3NUOBdEdkoIks9rB8u+3Axvf8H8+X+AxinqgfB+XIHxnooM1z24zdwfqF5crzPwmC61921tKyXrq/hsP/OA8pUdXcv6325/7wyEoNePDzWc4yoN2UGnYhEA68AD6hqXY/Vn+N0R8wG/gf4yxBXb76qzgEuB+4RkfN7rPf5PhSRMOAa4CUPq329/7w1HPbjD4AOYHkvRY73WRgsvwemAFnAQZzukZ58vv+Am+m7Ne+r/ee1kRj0xUBqt/spQMlJlBlUIhKKE/LLVfXVnutVtU5VG9y3VwGhIpI4VPVT1RL333LgNZyfyN35fB/i/Mf5XFXLeq7w9f5zK+vqznL/LfdQxqf7UUS+BlwF3KruDuWevPgsDApVLVPVTlV1AX/o5XV9vf9CgOuBF3sr46v9dyJGYtCvB6aKSIa7xbcYWNmjzErgq+6RI/OA2q6f2EPB3af3FLBdVX/TS5nx7nKISA7Ov0XVENUvSkRium7jHLTb0qOYT/ehW68tKV/uv25WAl9z3/4a8LqHMt58XgeFiCwEHgSuUdWmXsp481kYrPp1P+ZzXS+v67P95/YlYIeqFnta6cv9d0J8fTT4ZBacESG7cI7G/8D92F3AXe7bAjzmXr8ZyB7i+p2L8/NyE5DrXq7oUcd7ga04owg+Bc4ZwvpNdr9unrsOw3EfRuIE95huj/ls/+F84RwE2nFamXcACcB7wG7333h32YnAqr4+r0NUv3yc/u2uz+DjPevX22dhiOr3J/dnaxNOeE8YTvvP/fjTXZ+5bmWHfP/1d7EpEIwxxs+NxK4bY4wxJ8CC3hhj/JwFvTHG+DkLemOM8XMW9MYY4+cs6I0xxs9Z0BtjjJ/7/+RIQwb0uE99AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def show_history(history):\n",
    "    plt.plot(history.history['accuracy'], label='train')\n",
    "    plt.plot(history.history['val_accuracy'], label='valid')\n",
    "    plt.legend()\n",
    "\n",
    "show_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d1aa96",
   "metadata": {},
   "source": [
    "### Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c433910f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/jamm/Documents/Github/AI/DeepLearning_CNN/Fashion_MNIST\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "98117458",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "current_path = os.getcwd()\n",
    "callback_path = os.path.join(current_path, 'callbacks')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6f4c72",
   "metadata": {},
   "source": [
    "### ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d0d5b89",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "Epoch 1/10\n",
      " 18/399 [>.............................] - ETA: 2s - loss: 1.6893 - accuracy: 0.4579"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-13 14:35:49.243258: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "395/399 [============================>.] - ETA: 0s - loss: 0.6101 - accuracy: 0.7908"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-13 14:35:51.543052: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "399/399 [==============================] - 3s 6ms/step - loss: 0.6092 - accuracy: 0.7911 - val_loss: 0.4579 - val_accuracy: 0.8372\n",
      "Epoch 2/10\n",
      "399/399 [==============================] - 2s 6ms/step - loss: 0.4186 - accuracy: 0.8523 - val_loss: 0.4234 - val_accuracy: 0.8450\n",
      "Epoch 3/10\n",
      "399/399 [==============================] - 2s 6ms/step - loss: 0.3780 - accuracy: 0.8659 - val_loss: 0.3941 - val_accuracy: 0.8590\n",
      "\n",
      "Epoch 00003: val_loss improved from inf to 0.39408, saving model to /Users/jamm/Documents/Github/AI/DeepLearning_CNN/Fashion_MNIST/callbacks/weights.03-0.39.hdf5\n",
      "Epoch 4/10\n",
      "399/399 [==============================] - 2s 6ms/step - loss: 0.3500 - accuracy: 0.8748 - val_loss: 0.3852 - val_accuracy: 0.8593\n",
      "Epoch 5/10\n",
      "399/399 [==============================] - 2s 6ms/step - loss: 0.3350 - accuracy: 0.8783 - val_loss: 0.3501 - val_accuracy: 0.8762\n",
      "Epoch 6/10\n",
      "399/399 [==============================] - 2s 6ms/step - loss: 0.3151 - accuracy: 0.8862 - val_loss: 0.3385 - val_accuracy: 0.8771\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.39408 to 0.33847, saving model to /Users/jamm/Documents/Github/AI/DeepLearning_CNN/Fashion_MNIST/callbacks/weights.06-0.34.hdf5\n",
      "Epoch 7/10\n",
      "399/399 [==============================] - 2s 6ms/step - loss: 0.3018 - accuracy: 0.8907 - val_loss: 0.3572 - val_accuracy: 0.8680\n",
      "Epoch 8/10\n",
      "399/399 [==============================] - 2s 6ms/step - loss: 0.2953 - accuracy: 0.8936 - val_loss: 0.3333 - val_accuracy: 0.8802\n",
      "Epoch 9/10\n",
      "399/399 [==============================] - 2s 6ms/step - loss: 0.2811 - accuracy: 0.8979 - val_loss: 0.3376 - val_accuracy: 0.8806\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.33847 to 0.33755, saving model to /Users/jamm/Documents/Github/AI/DeepLearning_CNN/Fashion_MNIST/callbacks/weights.09-0.34.hdf5\n",
      "Epoch 10/10\n",
      "399/399 [==============================] - 2s 6ms/step - loss: 0.2745 - accuracy: 0.8998 - val_loss: 0.3339 - val_accuracy: 0.8789\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "model = create_model()\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(0.001), loss='categorical_crossentropy', metrics=['accuracy']\n",
    ")\n",
    "\n",
    "mcp_cb = ModelCheckpoint(filepath=callback_path + '/weights.{epoch:02d}-{val_loss:.2f}.hdf5', \n",
    "                        monitor='val_loss',\n",
    "                         save_best_only=True,\n",
    "                         save_weights_only=True,\n",
    "                         mode='min',\n",
    "                         period=3,\n",
    "                         verbose=1\n",
    "                        )\n",
    "\n",
    "history = model.fit(\n",
    "    x=tr_images,\n",
    "    y=tr_oh_labels,\n",
    "    batch_size=128,\n",
    "    epochs=10,\n",
    "    validation_data=(val_images, val_oh_labels),\n",
    "    callbacks=[mcp_cb]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3cfb9684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/jamm/Documents/Github/AI/DeepLearning_CNN/Fashion_MNIST/callbacks\n"
     ]
    }
   ],
   "source": [
    "%cd /Users/jamm/Documents/Github/AI/DeepLearning_CNN/Fashion_MNIST/callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5c0f639f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/jamm/Documents/Github/AI/DeepLearning_CNN/Fashion_MNIST/callbacks\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f1f0c65d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 2016\r\n",
      "5527260 drwxr-xr-x  5 jamm  staff     160  8 13 14:36 \u001b[34m.\u001b[m\u001b[m\r\n",
      "5318448 drwxr-xr-x  9 jamm  staff     288  8 13 14:35 \u001b[34m..\u001b[m\u001b[m\r\n",
      "5636821 -rw-r--r--  1 jamm  staff  343264  8 13 14:35 weights.03-0.39.hdf5\r\n",
      "5636850 -rw-r--r--  1 jamm  staff  343264  8 13 14:36 weights.06-0.34.hdf5\r\n",
      "5636879 -rw-r--r--  1 jamm  staff  343264  8 13 14:36 weights.09-0.34.hdf5\r\n"
     ]
    }
   ],
   "source": [
    "!ls -lia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6f6c75e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf weight*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cf9c7055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 0\r\n",
      "5527260 drwxr-xr-x  2 jamm  staff   64  8 13 14:37 \u001b[34m.\u001b[m\u001b[m\r\n",
      "5318448 drwxr-xr-x  9 jamm  staff  288  8 13 14:35 \u001b[34m..\u001b[m\u001b[m\r\n"
     ]
    }
   ],
   "source": [
    "!ls -lia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9cd379e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/jamm/Documents/Github/AI/DeepLearning_CNN/Fashion_MNIST\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aec3728b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/jamm/Documents/Github/AI/DeepLearning_CNN/Fashion_MNIST\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18415137",
   "metadata": {},
   "source": [
    "### ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1e6ee12b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      " 18/399 [>.............................] - ETA: 2s - loss: 1.8004 - accuracy: 0.3646"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-13 14:41:42.364434: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "397/399 [============================>.] - ETA: 0s - loss: 0.6084 - accuracy: 0.7910"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-13 14:41:44.659944: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "399/399 [==============================] - 3s 6ms/step - loss: 0.6079 - accuracy: 0.7911 - val_loss: 0.4855 - val_accuracy: 0.8262\n",
      "Epoch 2/30\n",
      "399/399 [==============================] - 2s 6ms/step - loss: 0.4181 - accuracy: 0.8545 - val_loss: 0.4086 - val_accuracy: 0.8569\n",
      "Epoch 3/30\n",
      "399/399 [==============================] - 2s 6ms/step - loss: 0.3742 - accuracy: 0.8675 - val_loss: 0.3876 - val_accuracy: 0.8624\n",
      "Epoch 4/30\n",
      "399/399 [==============================] - 2s 6ms/step - loss: 0.3449 - accuracy: 0.8761 - val_loss: 0.3560 - val_accuracy: 0.8722\n",
      "Epoch 5/30\n",
      "399/399 [==============================] - 3s 6ms/step - loss: 0.3233 - accuracy: 0.8834 - val_loss: 0.3654 - val_accuracy: 0.8682\n",
      "Epoch 6/30\n",
      "399/399 [==============================] - 2s 6ms/step - loss: 0.3105 - accuracy: 0.8883 - val_loss: 0.3415 - val_accuracy: 0.8798\n",
      "Epoch 7/30\n",
      "399/399 [==============================] - 2s 6ms/step - loss: 0.3007 - accuracy: 0.8900 - val_loss: 0.3260 - val_accuracy: 0.8827\n",
      "Epoch 8/30\n",
      "399/399 [==============================] - 2s 6ms/step - loss: 0.2905 - accuracy: 0.8948 - val_loss: 0.3504 - val_accuracy: 0.8730\n",
      "Epoch 9/30\n",
      "399/399 [==============================] - 2s 6ms/step - loss: 0.2787 - accuracy: 0.8978 - val_loss: 0.3327 - val_accuracy: 0.8807\n",
      "Epoch 10/30\n",
      "399/399 [==============================] - 2s 6ms/step - loss: 0.2700 - accuracy: 0.9017 - val_loss: 0.3104 - val_accuracy: 0.8869\n",
      "Epoch 11/30\n",
      "399/399 [==============================] - 2s 6ms/step - loss: 0.2603 - accuracy: 0.9049 - val_loss: 0.3415 - val_accuracy: 0.8760\n",
      "Epoch 12/30\n",
      "399/399 [==============================] - 2s 6ms/step - loss: 0.2553 - accuracy: 0.9056 - val_loss: 0.3379 - val_accuracy: 0.8793\n",
      "Epoch 13/30\n",
      "399/399 [==============================] - 2s 6ms/step - loss: 0.2496 - accuracy: 0.9072 - val_loss: 0.3331 - val_accuracy: 0.8804\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
      "Epoch 14/30\n",
      "399/399 [==============================] - 2s 6ms/step - loss: 0.2193 - accuracy: 0.9203 - val_loss: 0.2993 - val_accuracy: 0.8926\n",
      "Epoch 15/30\n",
      "399/399 [==============================] - 2s 6ms/step - loss: 0.2147 - accuracy: 0.9211 - val_loss: 0.2972 - val_accuracy: 0.8937\n",
      "Epoch 16/30\n",
      "399/399 [==============================] - 2s 6ms/step - loss: 0.2106 - accuracy: 0.9234 - val_loss: 0.2961 - val_accuracy: 0.8957\n",
      "Epoch 17/30\n",
      "399/399 [==============================] - 2s 6ms/step - loss: 0.2080 - accuracy: 0.9238 - val_loss: 0.2963 - val_accuracy: 0.8929\n",
      "Epoch 18/30\n",
      "399/399 [==============================] - 3s 6ms/step - loss: 0.2059 - accuracy: 0.9240 - val_loss: 0.3024 - val_accuracy: 0.8930\n",
      "Epoch 19/30\n",
      "399/399 [==============================] - 2s 6ms/step - loss: 0.2034 - accuracy: 0.9254 - val_loss: 0.2998 - val_accuracy: 0.8949\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
      "Epoch 20/30\n",
      "399/399 [==============================] - 2s 6ms/step - loss: 0.1922 - accuracy: 0.9307 - val_loss: 0.2950 - val_accuracy: 0.8963\n",
      "Epoch 21/30\n",
      "399/399 [==============================] - 3s 6ms/step - loss: 0.1913 - accuracy: 0.9308 - val_loss: 0.2945 - val_accuracy: 0.8957\n",
      "Epoch 22/30\n",
      "399/399 [==============================] - 3s 6ms/step - loss: 0.1899 - accuracy: 0.9308 - val_loss: 0.2955 - val_accuracy: 0.8958\n",
      "Epoch 23/30\n",
      "399/399 [==============================] - 2s 6ms/step - loss: 0.1889 - accuracy: 0.9319 - val_loss: 0.2929 - val_accuracy: 0.8968\n",
      "Epoch 24/30\n",
      "399/399 [==============================] - 2s 6ms/step - loss: 0.1877 - accuracy: 0.9324 - val_loss: 0.2960 - val_accuracy: 0.8954\n",
      "Epoch 25/30\n",
      "399/399 [==============================] - 2s 6ms/step - loss: 0.1865 - accuracy: 0.9338 - val_loss: 0.2967 - val_accuracy: 0.8962\n",
      "Epoch 26/30\n",
      "399/399 [==============================] - 2s 6ms/step - loss: 0.1855 - accuracy: 0.9332 - val_loss: 0.2947 - val_accuracy: 0.8968\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\n",
      "Epoch 27/30\n",
      "399/399 [==============================] - 2s 6ms/step - loss: 0.1824 - accuracy: 0.9349 - val_loss: 0.2957 - val_accuracy: 0.8966\n",
      "Epoch 28/30\n",
      "399/399 [==============================] - 2s 6ms/step - loss: 0.1818 - accuracy: 0.9350 - val_loss: 0.2945 - val_accuracy: 0.8969\n",
      "Epoch 29/30\n",
      "399/399 [==============================] - 2s 6ms/step - loss: 0.1815 - accuracy: 0.9353 - val_loss: 0.2951 - val_accuracy: 0.8967\n",
      "\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 8.100000013655517e-06.\n",
      "Epoch 30/30\n",
      "399/399 [==============================] - 2s 6ms/step - loss: 0.1804 - accuracy: 0.9357 - val_loss: 0.2949 - val_accuracy: 0.8964\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "model = create_model()\n",
    "model.compile(optimizer=Adam(0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "rlr_cb = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.3,\n",
    "    patience=3,\n",
    "    mode='min',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    x=tr_images,\n",
    "    y=tr_oh_labels,\n",
    "    batch_size=128,\n",
    "    epochs=30,\n",
    "    validation_data=(val_images, val_oh_labels),\n",
    "    callbacks=[rlr_cb]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e742638c",
   "metadata": {},
   "source": [
    "### EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b00ab662",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      " 18/399 [>.............................] - ETA: 2s - loss: 1.6156 - accuracy: 0.4531"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-13 14:48:12.922029: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "395/399 [============================>.] - ETA: 0s - loss: 0.5911 - accuracy: 0.7973"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-13 14:48:15.218097: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "399/399 [==============================] - 3s 6ms/step - loss: 0.5898 - accuracy: 0.7976 - val_loss: 0.4563 - val_accuracy: 0.8406\n",
      "Epoch 2/30\n",
      "399/399 [==============================] - 2s 6ms/step - loss: 0.4125 - accuracy: 0.8541 - val_loss: 0.4201 - val_accuracy: 0.8502\n",
      "Epoch 3/30\n",
      "399/399 [==============================] - 2s 6ms/step - loss: 0.3754 - accuracy: 0.8657 - val_loss: 0.3778 - val_accuracy: 0.8610\n",
      "Epoch 4/30\n",
      "399/399 [==============================] - 2s 6ms/step - loss: 0.3506 - accuracy: 0.8730 - val_loss: 0.3603 - val_accuracy: 0.8679\n",
      "Epoch 5/30\n",
      "399/399 [==============================] - 2s 6ms/step - loss: 0.3358 - accuracy: 0.8795 - val_loss: 0.3577 - val_accuracy: 0.8661\n",
      "Epoch 6/30\n",
      "399/399 [==============================] - 2s 6ms/step - loss: 0.3201 - accuracy: 0.8838 - val_loss: 0.3417 - val_accuracy: 0.8748\n",
      "Epoch 7/30\n",
      "399/399 [==============================] - 2s 6ms/step - loss: 0.3052 - accuracy: 0.8883 - val_loss: 0.3382 - val_accuracy: 0.8770\n",
      "Epoch 8/30\n",
      "399/399 [==============================] - 2s 6ms/step - loss: 0.2941 - accuracy: 0.8919 - val_loss: 0.3401 - val_accuracy: 0.8768\n",
      "Epoch 9/30\n",
      "399/399 [==============================] - 2s 6ms/step - loss: 0.2843 - accuracy: 0.8953 - val_loss: 0.3477 - val_accuracy: 0.8779\n",
      "Epoch 10/30\n",
      "399/399 [==============================] - 2s 6ms/step - loss: 0.2753 - accuracy: 0.8988 - val_loss: 0.3292 - val_accuracy: 0.8794\n",
      "Epoch 11/30\n",
      "399/399 [==============================] - 2s 6ms/step - loss: 0.2610 - accuracy: 0.9037 - val_loss: 0.3132 - val_accuracy: 0.8887\n",
      "Epoch 12/30\n",
      "399/399 [==============================] - 2s 6ms/step - loss: 0.2592 - accuracy: 0.9038 - val_loss: 0.3199 - val_accuracy: 0.8830\n",
      "Epoch 13/30\n",
      "399/399 [==============================] - 2s 6ms/step - loss: 0.2509 - accuracy: 0.9076 - val_loss: 0.3164 - val_accuracy: 0.8890\n",
      "Epoch 14/30\n",
      "399/399 [==============================] - 2s 6ms/step - loss: 0.2429 - accuracy: 0.9109 - val_loss: 0.3075 - val_accuracy: 0.8882\n",
      "Epoch 15/30\n",
      "399/399 [==============================] - 2s 6ms/step - loss: 0.2346 - accuracy: 0.9130 - val_loss: 0.3232 - val_accuracy: 0.8832\n",
      "Epoch 16/30\n",
      "399/399 [==============================] - 2s 6ms/step - loss: 0.2267 - accuracy: 0.9160 - val_loss: 0.3324 - val_accuracy: 0.8819\n",
      "Epoch 17/30\n",
      "399/399 [==============================] - 2s 6ms/step - loss: 0.2228 - accuracy: 0.9174 - val_loss: 0.3116 - val_accuracy: 0.8923\n",
      "Epoch 00017: early stopping\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "model = create_model()\n",
    "model.compile(optimizer=Adam(0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "ely_cb = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=3,\n",
    "    mode='min',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    x=tr_images,\n",
    "    y=tr_oh_labels,\n",
    "    batch_size=128,\n",
    "    epochs=30,\n",
    "    validation_data=(val_images, val_oh_labels),\n",
    "    callbacks=[ely_cb]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7edad59",
   "metadata": {},
   "source": [
    "### Callbacks Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f89c70af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "Epoch 1/40\n",
      " 19/399 [>.............................] - ETA: 2s - loss: 1.6796 - accuracy: 0.4630"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-13 15:00:02.365632: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "395/399 [============================>.] - ETA: 0s - loss: 0.6109 - accuracy: 0.7910"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-13 15:00:04.654883: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "399/399 [==============================] - 3s 6ms/step - loss: 0.6092 - accuracy: 0.7916 - val_loss: 0.5187 - val_accuracy: 0.8107\n",
      "Epoch 2/40\n",
      "399/399 [==============================] - 2s 6ms/step - loss: 0.4240 - accuracy: 0.8517 - val_loss: 0.4382 - val_accuracy: 0.8404\n",
      "Epoch 3/40\n",
      "399/399 [==============================] - 2s 6ms/step - loss: 0.3842 - accuracy: 0.8629 - val_loss: 0.3783 - val_accuracy: 0.8658\n",
      "Epoch 4/40\n",
      "399/399 [==============================] - 3s 6ms/step - loss: 0.3550 - accuracy: 0.8727 - val_loss: 0.3759 - val_accuracy: 0.8658\n",
      "Epoch 5/40\n",
      "399/399 [==============================] - 2s 6ms/step - loss: 0.3324 - accuracy: 0.8805 - val_loss: 0.3463 - val_accuracy: 0.8783\n",
      "Epoch 6/40\n",
      "399/399 [==============================] - 2s 6ms/step - loss: 0.3186 - accuracy: 0.8837 - val_loss: 0.3574 - val_accuracy: 0.8676\n",
      "Epoch 7/40\n",
      "399/399 [==============================] - 2s 6ms/step - loss: 0.3033 - accuracy: 0.8903 - val_loss: 0.3644 - val_accuracy: 0.8693\n",
      "Epoch 8/40\n",
      "399/399 [==============================] - 2s 6ms/step - loss: 0.2937 - accuracy: 0.8922 - val_loss: 0.3240 - val_accuracy: 0.8823\n",
      "Epoch 9/40\n",
      "399/399 [==============================] - 2s 6ms/step - loss: 0.2832 - accuracy: 0.8963 - val_loss: 0.3493 - val_accuracy: 0.8710\n",
      "Epoch 10/40\n",
      "399/399 [==============================] - 2s 6ms/step - loss: 0.2724 - accuracy: 0.9008 - val_loss: 0.3677 - val_accuracy: 0.8717\n",
      "Epoch 11/40\n",
      "399/399 [==============================] - 2s 6ms/step - loss: 0.2644 - accuracy: 0.9028 - val_loss: 0.3471 - val_accuracy: 0.8762\n",
      "Epoch 12/40\n",
      "399/399 [==============================] - 2s 6ms/step - loss: 0.2566 - accuracy: 0.9053 - val_loss: 0.3240 - val_accuracy: 0.8848\n",
      "Epoch 13/40\n",
      "399/399 [==============================] - 2s 6ms/step - loss: 0.2489 - accuracy: 0.9082 - val_loss: 0.3148 - val_accuracy: 0.8849\n",
      "Epoch 14/40\n",
      "399/399 [==============================] - 2s 6ms/step - loss: 0.2420 - accuracy: 0.9099 - val_loss: 0.3164 - val_accuracy: 0.8851\n",
      "Epoch 15/40\n",
      "399/399 [==============================] - 2s 6ms/step - loss: 0.2353 - accuracy: 0.9132 - val_loss: 0.3212 - val_accuracy: 0.8821\n",
      "Epoch 16/40\n",
      "399/399 [==============================] - 3s 6ms/step - loss: 0.2315 - accuracy: 0.9136 - val_loss: 0.3249 - val_accuracy: 0.8849\n",
      "Epoch 17/40\n",
      "399/399 [==============================] - 3s 6ms/step - loss: 0.2284 - accuracy: 0.9152 - val_loss: 0.3136 - val_accuracy: 0.8873\n",
      "Epoch 18/40\n",
      "399/399 [==============================] - 3s 6ms/step - loss: 0.2168 - accuracy: 0.9199 - val_loss: 0.3141 - val_accuracy: 0.8877\n",
      "Epoch 19/40\n",
      "399/399 [==============================] - 2s 6ms/step - loss: 0.2129 - accuracy: 0.9213 - val_loss: 0.3082 - val_accuracy: 0.8892\n",
      "Epoch 20/40\n",
      "399/399 [==============================] - 2s 6ms/step - loss: 0.2095 - accuracy: 0.9224 - val_loss: 0.3362 - val_accuracy: 0.8836\n",
      "Epoch 21/40\n",
      "399/399 [==============================] - 2s 6ms/step - loss: 0.2049 - accuracy: 0.9241 - val_loss: 0.3215 - val_accuracy: 0.8846\n",
      "Epoch 22/40\n",
      "399/399 [==============================] - 2s 6ms/step - loss: 0.1960 - accuracy: 0.9273 - val_loss: 0.3254 - val_accuracy: 0.8863\n",
      "Epoch 23/40\n",
      "399/399 [==============================] - 2s 6ms/step - loss: 0.1951 - accuracy: 0.9271 - val_loss: 0.3223 - val_accuracy: 0.8910\n",
      "Epoch 24/40\n",
      "399/399 [==============================] - 2s 6ms/step - loss: 0.1895 - accuracy: 0.9307 - val_loss: 0.3279 - val_accuracy: 0.8848\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
      "Epoch 25/40\n",
      "399/399 [==============================] - 2s 6ms/step - loss: 0.1597 - accuracy: 0.9419 - val_loss: 0.3144 - val_accuracy: 0.8944\n",
      "Epoch 26/40\n",
      "399/399 [==============================] - 2s 6ms/step - loss: 0.1562 - accuracy: 0.9441 - val_loss: 0.3106 - val_accuracy: 0.8927\n",
      "Epoch 00026: early stopping\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "\n",
    "model = create_model()\n",
    "model.compile(optimizer=Adam(0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "mcp_cb = ModelCheckpoint(\n",
    "    filepath=callback_path + '/weights.{epoch:02d}-{val_loss:.2f}.hdf5',\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True,\n",
    "    mode='min',\n",
    "    period=1,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "rlr_cb = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.3,\n",
    "    patience=5,\n",
    "    mode='min',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "ely_cb = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=7,\n",
    "    mode='min',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    x=tr_images,\n",
    "    y=tr_oh_labels,\n",
    "    batch_size=128,\n",
    "    epochs=40,\n",
    "    validation_data=(val_images, val_oh_labels),\n",
    "    callbacks=[mcp_cb, rlr_cb, ely_cb]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2683f5fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/jamm/Documents/Github/AI/DeepLearning_CNN/Fashion_MNIST\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "65845400",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/jamm/Documents/Github/AI/DeepLearning_CNN/Fashion_MNIST/callbacks\n"
     ]
    }
   ],
   "source": [
    "%cd /Users/jamm/Documents/Github/AI/DeepLearning_CNN/Fashion_MNIST/callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a8615114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/jamm/Documents/Github/AI/DeepLearning_CNN/Fashion_MNIST/callbacks\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "24cdfe7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 6048\r\n",
      "5527260 drwxr-xr-x  11 jamm  staff     352  8 13 15:00 \u001b[34m.\u001b[m\u001b[m\r\n",
      "5318448 drwxr-xr-x   9 jamm  staff     288  8 13 15:01 \u001b[34m..\u001b[m\u001b[m\r\n",
      "5641663 -rw-r--r--   1 jamm  staff  343440  8 13 15:00 weights.01-0.52.hdf5\r\n",
      "5641665 -rw-r--r--   1 jamm  staff  343440  8 13 15:00 weights.02-0.44.hdf5\r\n",
      "5641675 -rw-r--r--   1 jamm  staff  343440  8 13 15:00 weights.03-0.38.hdf5\r\n",
      "5641696 -rw-r--r--   1 jamm  staff  343440  8 13 15:00 weights.04-0.38.hdf5\r\n",
      "5641701 -rw-r--r--   1 jamm  staff  343440  8 13 15:00 weights.05-0.35.hdf5\r\n",
      "5641729 -rw-r--r--   1 jamm  staff  343440  8 13 15:00 weights.08-0.32.hdf5\r\n",
      "5641766 -rw-r--r--   1 jamm  staff  343440  8 13 15:00 weights.13-0.31.hdf5\r\n",
      "5641796 -rw-r--r--   1 jamm  staff  343440  8 13 15:00 weights.17-0.31.hdf5\r\n",
      "5641800 -rw-r--r--   1 jamm  staff  343440  8 13 15:00 weights.19-0.31.hdf5\r\n"
     ]
    }
   ],
   "source": [
    "!ls -lia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "463842c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm weight*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "63b002ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 0\r\n",
      "5527260 drwxr-xr-x  2 jamm  staff   64  8 13 15:02 \u001b[34m.\u001b[m\u001b[m\r\n",
      "5318448 drwxr-xr-x  9 jamm  staff  288  8 13 15:01 \u001b[34m..\u001b[m\u001b[m\r\n"
     ]
    }
   ],
   "source": [
    "!ls -lia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7a176000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/jamm/Documents/Github/AI/DeepLearning_CNN/Fashion_MNIST\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40951750",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
